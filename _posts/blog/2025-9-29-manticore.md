---
layout: post
title: 'Introducing Manticore: Building Pretrained Hybrids with MAD Skills'
date: 2025-9-29
permalink: /posts/2025/9/manticore/
tags:
  - Research
  - Hybrids
  - AutoML
  - NAS
categories: [hybrids, automl]
thumbnail: /img/blog/manticore/manticore_banner.png
---

Language model architectures are changing‚Äîor at least, the number of possible language model architecture choices is increasing. Transformers remain the foundation of most state-of-the-art systems, but the ecosystem is diversifying rapidly. Alternatives such as Mamba bring linear-time inference and new capabilities, while recurrent and state-space models are enjoying a renaissance. What's missing is a clear "winner." Transformers perform well on many tasks, but not all. They excel at in-context learning and parallel processing but struggle with very long sequences due to quadratic attention costs. Mamba shines in some settings‚Äîparticularly those requiring efficient long-range modeling‚Äîbut lags in others where the inductive biases of attention matter. This uneven landscape has naturally led researchers to hybrids: models that combine the strengths of multiple architectures. If Transformers handle in-context learning and Mamba handles long-range dependencies, then perhaps a hybrid can do both. Early demonstrations, like MambaFormer and NVIDIA‚Äôs Nemotron Nano, show that the idea works. But building them by hand is costly and fragile. The design space is enormous‚Äîwhich blocks go where? how many of each?‚Äîand manual exploration doesn‚Äôt scale. Most hybrids need to be trained from scratch because different architectures operate in incompatible feature spaces, and they rarely take advantage of the massive pool of pretrained models that already exist. With *Manticore*, we wanted to change that. Our framework automatically builds hybrids out of pretrained components. Instead of discarding years of compute and data, we reuse existing models, align their representations with lightweight projectors, and let learnable weights decide how much each model contributes. The result is a practical way to create pretrained hybrids, without incurring the costs that have held hybrids back.

![Manticore hybrid design](/img/blog/manticore/manticore_banner.png "Manticore hybrid design")

### Why Hybrids Are Hard

To understand the need for Manticore, it helps to look more closely at why hybrids have been difficult to build in practice. The first obstacle is the sheer size of the design space. Once you allow blocks from different architectures to be combined, the number of possible hybrids explodes combinatorially. How many Transformer layers should you use? How often should you alternate them with Mamba blocks? Do you put them at the beginning of the model, at the end, or scatter them throughout? Should you use residual connections between hybrid blocks? Each choice creates new branches in an already vast tree of possibilities. Without a systematic way of navigating this space, most designs come down to either brute force or intuition, both of which are expensive and brittle. The second obstacle is compatibility. Pretrained models are incredibly valuable‚Äîthey encode billions of tokens‚Äô worth of learned patterns‚Äîbut the internal ‚Äúlanguages‚Äù spoken by their blocks are different. The hidden states of a Transformer and those of a state-space model live in different representational spaces, often with different dimensions. If you try to stitch them together directly, the representations won‚Äôt align and they will fail to communicate. A Transformer block expects certain distributional properties in its inputs that a Mamba block‚Äôs outputs don‚Äôt naturally provide, and vice versa. That means most existing hybrids have had to throw away pretrained weights and start from scratch, treating the problem as pure architecture search rather than component reuse. And then there is the cost. Training a large model from scratch is already enormously expensive‚Äîoften millions of dollars in compute for frontier models‚Äîand training multiple hybrids just to see what works is out of reach for most groups. Even when the potential benefits are clear, the barrier has been too high. This is particularly frustrating because the pretrained models we‚Äôd want to combine already exist and are publicly available. 

![Component models and alignment](/img/blog/manticore/component_models.png)  

### A Different Approach

Manticore tackles these challenges head on. It is named after the Persian mythological creature‚Äîa human, a lion, and a scorpion combined into something new. Our framework plays the same trick in the model space, mixing pretrained blocks into functional hybrids. The key idea is that you don‚Äôt need a brand new architecture or a complicated merging procedure. Instead, you add just two ingredients: adapters that make different models‚Äô features compatible, and weights that decide how to blend them. This is similar in spirit to model merging techniques, but instead of interpolating weights, we‚Äôre interpolating forward passes through different models‚Äô blocks.

*Projectors.* These are small linear layers with gated residuals that translate features from one architecture into a shared space. The gating mechanism is crucial‚Äîit allows the projector to learn when to apply the transformation and when to pass features through unchanged. We expected that aligning very different models might require complex transformations and large datasets, perhaps needing multiple layers or nonlinear activations. Instead, we discovered that linear adapters trained on just 100 million tokens of general text were enough. This is surprisingly small‚Äîless than 0.1% of what most foundation models see during pretraining. They are lightweight (adding minimal parameters compared to the component models), easy to train (converging in hours rather than weeks), and can be reused for many downstream tasks without retraining. Once you‚Äôve aligned GPT-Neo and Mamba, those projectors work across multiple fine-tuning tasks.

![Projectors diagram](/img/blog/manticore/projectors.png)  

*Mixture weights.* Once the features are in the same space, we still need to decide how much of each to keep. This is where neural architecture search comes in. Mixture weights, parameterized as a softmax over learnable scalars, blend the projected outputs from different component models. During training, NAS algorithms such as DARTS or GAEA optimize these weights jointly with the task loss. If one architecture is particularly helpful for the task, its weight increases. If it‚Äôs not contributing, the weight drops toward zero. The elegant part is that if the weights collapse to a single option‚Äîall mass on one component‚ÄîManticore reduces exactly to the original component model. That means our search space always contains the starting models and any possible hybrid in between, including manually designed hybrids like MambaFormer (which are just specific configurations of our mixture weights).

![Mixture weights diagram](/img/blog/manticore/mixture_weights.png)

Together, projectors and mixture weights form *Manticore blocks*. By chaining these blocks and dividing each component model‚Äôs layers evenly across them, we can connect entire pretrained models‚ÄîGPT-Neo, Pythia, Mamba‚Äîwithout discarding their weights. What emerges is not a handcrafted curiosity, but a systematic way of constructing hybrids. The framework is general: you can plug in any decoder-only language model that follows the standard embedding ‚Üí blocks ‚Üí head structure, and Manticore will figure out how to combine them.

### Using MAD as a Guide

Even with these tools, the question remains: which hybrids are worth building? Searching directly on large-scale tasks would defeat the purpose, since it would require massive computation‚Äîyou‚Äôd need to fine-tune many candidate hybrids to see which works best. This is where *MAD tasks*‚ÄîMechanistic Architecture Design‚Äîcome in. Originally introduced as synthetic ‚Äúunit tests‚Äù for language models, MAD tasks test specific capabilities: in-context recall (can the model retrieve a value associated with a key earlier in context?), selective copying (can it copy specific tokens while ignoring noise?), fuzzy recall (can it group semantically similar keys?), and memorization (can it learn fixed key-value pairs?). These tasks are predictive of scaling behavior on real data. In Manticore, they serve as proxies. We can train smaller hybrids on MAD tasks, observe which mixture weights work well across multiple synthetic benchmarks, and then transfer those insights to larger pretrained hybrids on real-world data. The intuition is that if a hybrid architecture works well on the fundamental capabilities tested by MAD, it‚Äôs likely to work well on complex downstream tasks that require those capabilities. In effect, MAD acts like a compass. It doesn‚Äôt tell you exactly which path to take, but it points toward regions of the search space that are promising. This lets us ‚Äúprogram‚Äù hybrids without ever touching the target dataset, saving enormous amounts of compute. For tasks where you can‚Äôt afford extensive architecture search‚Äîmaybe you have limited compute or the dataset is private‚ÄîMAD provides a way forward.

### What the Experiments Showed

We tested Manticore in three scenarios: fine-tuning pretrained hybrids, training hybrids from scratch, and programming hybrids without task-specific data. Across all three, we found that Manticore could discover or match strong hybrids, often with less compute than traditional approaches would require.

*Fine-tuning pretrained hybrids.* We combined Pythia-410M and Mamba-370M, replacing their embeddings and LM heads with shared components, and added projectors trained on 100M tokens of general text from FineWeb. We created a single Manticore block that mixed blocks from both models, then searched for mixture weights using DARTS. After search, we rewound the component models to their pretrained state and fine-tuned with frozen mixture weights. Fine-tuning these hybrids on Penn Treebank, Alpaca, and ELI5 showed consistent gains over the individual models. On Penn Treebank, the hybrid achieved a loss of **0.86**, compared to **0.91** for Pythia and **0.84** for Mamba‚Äîlanding between the two components but combining their complementary strengths. On Alpaca and ELI5, Manticore outperformed both, achieving **2.18** and **3.93** respectively versus Pythia‚Äôs **2.50** and **4.13**, with Mamba at **2.30** and **3.94**. The improvements were especially clear when the datasets were heterogeneous. On tasks mixing Spanish QA with English instructions, or Chinese QA with Alpaca, Manticore‚Äôs ability to dynamically leverage different components for different data slices became apparent. Where Pythia struggled with non-English text and Mamba struggled with instruction-following, the hybrid used both effectively. We also swept the mixture-weight space and found that NAS algorithms (DARTS, GAEA, DASH) reliably discovered the same high-performing regions, suggesting the search landscape is well-behaved.

*Training from scratch.* To see if the approach works without pretrained weights, we built hybrids from randomly initialized GPT-Neo and Mamba models on MAD tasks and Long Range Arena. On MAD tasks with harder parameters‚Äîlarger vocabularies, less training data‚ÄîManticore **matched or approached** manually designed hybrids such as MambaFormer and the Striped Hyena architecture. For example, on **Selective Copying** Manticore reached ~**0.017** (vs. **0.0005** for MambaFormer), while on **Memorization** Manticore was **8.94** (vs. **5.22** for MambaFormer)‚Äîlower is better here, so MambaFormer leads, but Manticore **outperformed the non-hybrid** GPT-Neo (**4.61**) and Mamba (**5.23**) configuration in the other MAD setting and remained competitive overall. On the Long Range Arena benchmark, Manticore improved over its component models on **4 of 5 tasks**. On **ListOps**, it achieved **38.7%** accuracy versus **37.9%** for GPT-Neo and **20.7%** for Mamba. On **Pathfinder**, it hit **91.5%** versus **89.4%** and **85.8%**. The one exception was **IMDb**, where Mamba (**87.7%**) substantially outperformed both GPT-Neo (**59.6%**) and Manticore (**72.4%**)‚Äîin this case, our search procedure didn‚Äôt fully recover the best single component, suggesting room for improvement in the NAS algorithm. On **Pathfinder-X**, which required sequence lengths beyond GPT-Neo‚Äôs 2048-token limit, Manticore achieved **75.5%** by automatically setting GPT-Neo‚Äôs mixture weight to zero and reducing to pure Mamba. This demonstrates a key feature: Manticore gracefully handles incompatible components by down-weighting them when necessary.

*Programming without task data.* Finally, we explored whether hybrids could be set up without touching the target task. Sometimes this was as simple as using metadata. If a task required long contexts beyond GPT-Neo‚Äôs capacity, we could drop it from the mixture entirely by setting its weight to zero‚Äîa form of ‚Äúprogramming‚Äù based on known architectural constraints. More interestingly, we trained smaller hybrids on MAD tasks, averaged their mixture weights across the synthetic benchmarks, and applied those to pretrained hybrids on **Penn Treebank completions**. This is the ‚Äúprogramming‚Äù workflow: search on cheap proxy tasks, transfer to expensive real tasks. On PTB completions‚Äîa synthetic dataset we created by prompting GPT-Neo and Mamba with PTB sentences‚Äîthese MAD-predicted weights landed in the same high-performing region found by full search. The resulting hybrid achieved a loss within a few hundredths of the best discovered by search, and when we visualized the loss landscape by sweeping mixture weights, the MAD trajectory (showing how weights evolved during MAD training) tracked the gradient of the PTB landscape despite being trained on completely different data. This suggests that MAD tasks capture something fundamental about how different architectures complement each other, at least for tasks close to the pretraining distribution. The technique worked less well for tasks far from pretraining (e.g., non-English datasets), where the loss landscapes became less correlated.

![Mixture-weight sweeps and MAD trajectory](/img/blog/manticore/sweeps.png)

### Tradeoffs

Manticore is not free. At inference, you need to run each component model, which increases cost compared to using just one. For a hybrid of two ~400M-parameter models, inference costs roughly **2√ó the FLOPs** of a single model. The projectors add negligible overhead‚Äîon the order of **<1%** of total FLOPs‚Äîso the cost scales linearly with the number of components. But this is still far cheaper than pretraining hybrids from scratch, which would require hundreds of billions of tokens and weeks of compute on large clusters. For many applications, the tradeoff is worth it: you avoid enormous pretraining costs while gaining flexibility to explore new combinations. If inference cost is paramount, you can discretize the mixture weights after search (setting them to one-hot vectors) and deploy only the selected components, though this can reduce performance slightly. Another tradeoff is search. While Manticore works with off-the-shelf NAS algorithms like DARTS, better hybrid-specific search methods could further improve results. We found that DARTS occasionally got stuck in local optima (as on IMDb), and that alternating updates between architecture and model parameters helped in some settings but not others. We see this as an opportunity for future work‚Äîdeveloping NAS algorithms tailored to the structure of hybrid language models‚Äînot a limitation of the framework. Finally, there‚Äôs the question of when hybrids help. Our results suggest they‚Äôre most useful when component models have complementary strengths, particularly on heterogeneous datasets. On homogeneous tasks where one architecture dominates, Manticore tends to recover that architecture, which is the right behavior but doesn‚Äôt provide gains.

### The Bigger Picture

The field no longer revolves around a single architecture. Transformers remain dominant, but recurrent and state-space models are advancing quickly, and more are likely to follow. New architectures like Griffin (combining recurrence and local attention) and Striped Hyena (mixing convolutions and attention) are appearing regularly. Instead of waiting for one to replace the others, Manticore suggests a different path: embrace the diversity. By reusing pretrained models, we preserve the massive investments already made‚Äîboth the compute spent on pretraining and the knowledge encoded in those models. By automatically searching over mixtures, we can explore new designs without human guesswork or exhaustive enumeration. By leaning on MAD tasks, we can anticipate which hybrids will succeed even before we see the target data. Hybrids have long promised a ‚Äúbest of all worlds‚Äù approach, but cost and complexity held them back. With Manticore, we hope to make hybrids practical tools for the community, not just interesting experiments. The framework is open and extensible‚Äînew component models can be added as they‚Äôre released, new search algorithms can be plugged in, and new proxy tasks beyond MAD can guide the process. We‚Äôre excited to see where the community takes this.

If you arrived at this page by scanning our QR code at COLM, and you stuck with it until the end, here are two of our finest cookies for your browser caching pleasure. üç™üç™

As always, if you‚Äôre interested in reading more about Manticore, please check out our paper!

- *Paper:* [Pretrained Hybrids with MAD Skills](https://openreview.net/forum?id=8xSbwT3763)  
- *Thread:* [X/Twitter link](https://x.com/nick11roberts/status/1944880136597475640)

Nicholas Roberts ([nick11roberts@cs.wisc.edu](mailto:nick11roberts@cs.wisc.edu)),  
Srinath Namburi ([namburisrinath@gmail.com](mailto:namburisrinath@gmail.com))
