<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="https://www.w3.org/2005/Atom">
  <channel>
    <title>Nicholas Roberts</title>
    <description>Drop me an email at nick.roberts.127.0.0.1@gmail.com</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 29 Sep 2025 18:41:02 +0000</pubDate>
    <lastBuildDate>Mon, 29 Sep 2025 18:41:02 +0000</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>Pretrained Hybrids with MAD Skills</title>
        <description>&lt;p&gt;Coming soon, stay tuned for updates!&lt;/p&gt;

&lt;p&gt;Nicholas Roberts &lt;a href=&quot;mailto:nick11roberts@cs.wisc.edu]&quot;&gt;nick11roberts@cs.wisc.edu&lt;/a&gt;, Srinath Namburi &lt;a href=&quot;mailto:namburisrinath@gmail.com&quot;&gt;namburisrinath@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/posts/2025/9/manticore/</link>
        <guid isPermaLink="true">http://localhost:4000/posts/2025/9/manticore/</guid>
        
        <category>Research</category>
        
        <category>Hybrids</category>
        
        <category>AutoML</category>
        
        <category>NAS</category>
        
        
        <category>hybrids</category>
        
        <category>automl</category>
        
      </item>
    
      <item>
        <title>AutoWS-Bench-101: Benchmarking Automated Weak Supervision on Diverse Tasks</title>
        <description>&lt;p&gt;It’s no secret that large-scale supervised machine learning is expensive and that one of the biggest challenges is in obtaining the labeled data required for training machine learning  models. 
Weak Supervision (WS) a popular and quite successful technique for reducing this need for labeled data. 
WS relies on access to noisy, heuristic functions that produce reasonable label guesses–these are called labeling functions, or LFs for short. 
Given a handful of these LFs, WS attempts to learn the relationships between the LFs and the true but &lt;em&gt;unobserved&lt;/em&gt; label–the component that does this is called the Label Model. 
WS is fairly easy to apply to text data, it’s harder to apply to data with more complex features. 
Automated Weak Supervision (AutoWS) solves this problem by instead &lt;em&gt;learning&lt;/em&gt; the LFs using a small amount of labeled data. 
The beauty of all of this is that WS and AutoWS can be combined with other ways of dealing with a lack of labeled data, like zero-shot learning with foundation models, or self-supervised learning. 
In this blog post, we will shed some light on AutoWS and explain the motivation behind our AutoWS-Bench-101 benchmark, the first-ever benchmark for AutoWS!&lt;/p&gt;

&lt;h1 id=&quot;weak-supervision-by-example-rotten-tomatoes&quot;&gt;Weak Supervision by example: Rotten Tomatoes&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://sprocketlab.github.io/images/blogposts/autowsbench101/ws.jpg&quot; alt=&quot;Weak Supervision Pipeline&quot; title=&quot;Weak Supervision Pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s step through a quick example of WS on movie review data… 
Here, the goal is to classify Rotten Tomatoes reviews as either “Fresh (+)” or “Rotten (-)”
Suppose we start off with three LFs, and for simplicity, we will use majority voting as our Label Model:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;LF1 returns “Fresh” if the movie review contains the word “amazing,” otherwise don’t vote,&lt;/li&gt;
  &lt;li&gt;LF2 returns “Rotten” if the movie review contains the word “nightmare,” otherwise don’t vote, and&lt;/li&gt;
  &lt;li&gt;LF3 returns “Fresh” or “Rotten” depending on the prediction of an off-the-shelf sentiment classifier.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now let’s apply these LFs to the following review of the 2019 movie Cats:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;At best, it’s an ambitious misfire. At worst, it’s straight-up nightmare fuel that will haunt generations. Enter into the world of the Jellicles at your own peril.&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Most people would probably assign this review the label “Rotten,” though since we’re doing WS, let’s check to see if our LFs agree… 
LF1 doesn’t vote because the word “amazing” does not appear in the text, LF2 votes “Rotten,” and for the sake of argument, suppose that LF3 also votes “Rotten.” 
Since we’re aggregating these LF outputs using majority vote, WS correctly labels this review as “Rotten.”&lt;/p&gt;

&lt;p&gt;The purpose of this example was twofold: first, if you aren’t familiar with WS, this example was hopefully illuminating. And second, that &lt;em&gt;it’s easy to write LFs for text data!&lt;/em&gt; 
The “features” that come with text (i.e., words) are more intuitive for humans to reason about, which makes it easier to come up with fairly general rules for text tasks.&lt;/p&gt;

&lt;p&gt;But what about data with more complex features, such as images? 
To our knowledge, traditional WS hasn’t even been applied to MNIST, because writing LFs from scratch for raw pixel data is simply not practical.&lt;/p&gt;

&lt;h1 id=&quot;from-ws-to-autows&quot;&gt;From WS to AutoWS&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://sprocketlab.github.io/images/blogposts/autowsbench101/autows-bench-101-banner.jpg&quot; alt=&quot;Automated Weak Supervision Pipeline&quot; title=&quot;Automated Weak Supervision Pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fortunately, dealing with more complex features only requires a few extra steps… 
The general idea is to &lt;em&gt;learn&lt;/em&gt; the LFs using a small set of labeled examples instead of writing the LFs by hand. 
This technique is called Automated Weak Supervision, or AutoWS, and the pipeline illustrating this process is shown in the above diagram. 
What makes traditional weak supervision difficult for things like images is that they are typically represented as tensors of pixel values, and it is challenging for a human to write explicit LFs on the pixel level to classify these data. 
Of course, this is true of other data types as well–including things like PDEs, which are often used for physics simulations, medical data, and featurized tabular data.&lt;/p&gt;

&lt;p&gt;The first step in most AutoWS techniques is to obtain a more useful representation of the complex data.
This is typically done by using some form of dimensionality reduction, by either using a classical technique such as PCA or by using an embedding obtained from a modern foundation model.&lt;/p&gt;

&lt;p&gt;Next, AutoWS techniques often use some small set of existing labeled examples to train simple models in this feature representation–these are called weak learners, and these will be used in place of the hand-designed LFs used in traditional WS.&lt;/p&gt;

&lt;p&gt;Finally, we proceed with the rest of the original WS pipeline by learning the parameters of the LM and we generate training data! Except now, we can generate training data for &lt;em&gt;much more complex and diverse domains, including a large variety of previously challenging scientific domains&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Armed with the two key takeaways from our deep dive into AutoWS methods,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;AutoWS methods mostly comprise two steps: feature representation, and obtaining weak learners&lt;/li&gt;
  &lt;li&gt;AutoWS methods unlock a huge variety of diverse applications for WS 
we developed our first-ever benchmark for AutoWS methods: AutoWS-Bench-101.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;autows-bench-101&quot;&gt;AutoWS-Bench-101&lt;/h1&gt;
&lt;p&gt;With AutoWS-Bench-101, we benchmark AutoWS methods using only 100 initial labeled examples, which gives our benchmark its name, as our goal is to generate the 101st label onward! 
We do so by applying the two previously-mentioned takeaways–we evaluate the cross product of a set of feature representation methods with a set of AutoWS methods, and we do so on a diverse set of applications.&lt;/p&gt;

&lt;p&gt;In particular, we tried a wide range of feature representation techniques, of varying complexity–simply using raw features, PCA, an ImageNet-trained ResNet-18, and features from CLIP–a modern foundation model. 
We plug each of these into a handful of AutoWS techniques, including &lt;a href=&quot;https://www.vldb.org/pvldb/vol12/p223-varma.pdf&quot;&gt;Snuba&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2012.06046&quot;&gt;Interactive Weak Supervision&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1903.04552&quot;&gt;GOGGLES&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our benchmark comprises three main categories of datases:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Image tasks&lt;/li&gt;
  &lt;li&gt;NLP tasks&lt;/li&gt;
  &lt;li&gt;diverse tasks&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the first category, we include MNIST, CIFAR-10, a &lt;a href=&quot;https://arxiv.org/abs/1801.10130&quot;&gt;spherically-projected version of MNIST&lt;/a&gt;, and MNIST with permuted pixels. 
Next, for backward compatibility with &lt;a href=&quot;https://arxiv.org/abs/2109.11377&quot;&gt;WRENCH&lt;/a&gt;, a benchmark for WS, we include three of the NLP datasets from their benchmark: YouTube, Yelp, and IMDb. 
Finally, we include three datasets from diverse application domains, where we think that AutoWS is quite promising: electrocardiograms (ECG), classifying the turbulence of a PDE (Navier-Stokes), and malware detection (EMBER).&lt;/p&gt;

&lt;h1 id=&quot;key-takeaways-from-autows-bench-101&quot;&gt;Key takeaways from AutoWS-Bench-101&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://sprocketlab.github.io/images/blogposts/autowsbench101/perfprof.png&quot; alt=&quot;Performance profile curves&quot; title=&quot;Performance Profile Curves&quot; /&gt;
The standard of evaluation for AutoWS-Bench-101 relies on &lt;a href=&quot;https://arxiv.org/abs/cs/0102001&quot;&gt;performance profile curves&lt;/a&gt;, which are a holistic way to evaluate different methods across various settings or “environments.” 
We won’t go into too many details of how these are computed here, and instead, I’ll refer you to &lt;a href=&quot;http://www.argmin.net/2018/03/26/performance-profiles/&quot;&gt;a nice blog post by Ben Recht&lt;/a&gt; on the topic. 
The key idea of performance profiles is that the higher curves are better for most tasks, or at least close to the best method for a given task, and the curves themselves are able to express situations in which a method is actually dramatically worse than the best method.&lt;/p&gt;

&lt;p&gt;Using performance profiles, we were able to see several interesting trends across our three categories of datases:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;few shot learning actually does better on vision and NLP tasks than many AutoWS methods, however, on diverse tasks, AutoWS catches up,&lt;/li&gt;
  &lt;li&gt;CLIP is very useful for vision tasks, but not for diverse or NLP tasks, in part due to a lack of compatibility,&lt;/li&gt;
  &lt;li&gt;an ImageNet-trained ResNet-18 model is surprisingly helpful for diverse tasks, despite being far afield from ImageNet.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using our benchmark, we also came away with these other key findings:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Foundation models &lt;strong&gt;are only helpful to AutoWS for in-distribution or related tasks,&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;LFs that output multiple classes &lt;strong&gt;might be better than class-specialized LFs,&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Foundation model usage can &lt;strong&gt;hurt coverage,&lt;/strong&gt; the fraction of points labeled by AutoWS.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more details about these findings, and our ablation studies of the various AutoWS methods that we tried, check out &lt;a href=&quot;https://arxiv.org/abs/2208.14362&quot;&gt;our paper!&lt;/a&gt; 😃&lt;/p&gt;

&lt;p&gt;And if you arrived at this page by scanning our QR code at NeurIPS, and you made it all the way here, here’s a cookie. 🍪&lt;/p&gt;

&lt;h1 id=&quot;whats-next-for-ws-and-autows-benchmarking-concluding-thoughts-and-ideas&quot;&gt;What’s next for WS and AutoWS benchmarking? Concluding thoughts and ideas…&lt;/h1&gt;
&lt;p&gt;We’re excited to add more functionality and methods to AutoWS-Bench-101! 
But beyond this benchmark and WRENCH, what is left to do? 
I mentioned before that WS and AutoWS can be combined with zero-shot learning with foundation models and self-supervised learning… 
But how do we find out which methods, or combination of methods, are actually the most useful for different types of tasks?&lt;/p&gt;

&lt;p&gt;I am personally excited about the idea leveraging community involvement to answer these big questions.
As an organizer of the &lt;a href=&quot;https://www.cs.cmu.edu/~automl-decathlon-22/&quot;&gt;AutoML Decathlon&lt;/a&gt; competition at NeurIPS 2022, one idea that I’m excited about is to run a Weak Supervision &lt;em&gt;coopetition&lt;/em&gt;–a cooperative competition. 
The idea behind this is to solcit LFs for a set of diverse tasks with mostly unobserved labeles from the community–and cooperatively solve the challenge of programmatically-labeleing large datasets via a Kaggle-like interface. 
I like this idea because it is goal-driven: the community must find a way to label these datasets by any means necessary, and everyone can help one another out by contributing to a shared GitHub repository. 
I think that this could be similar to something like &lt;a href=&quot;https://github.com/google/BIG-bench&quot;&gt;Google Big-Bench&lt;/a&gt;, with the promise of publishing a paper with many authors, and the eternal glory of having contributed to a large-scale (possibly registered + peer-reviewed) supervision experiment.&lt;/p&gt;

&lt;p&gt;Whether the next steps for WS benchmarking end up being related to this idea of a coopetition or something entirely different, I’m super excited to see where we go next with reducing the need for labeled data!&lt;/p&gt;

&lt;p&gt;Nicholas Roberts &lt;a href=&quot;mailto:nick11roberts@cs.wisc.edu]&quot;&gt;nick11roberts@cs.wisc.edu&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/posts/2022/11/autows-bench-101/</link>
        <guid isPermaLink="true">http://localhost:4000/posts/2022/11/autows-bench-101/</guid>
        
        <category>Research</category>
        
        <category>Weak Supervision</category>
        
        <category>AutoML</category>
        
        <category>Diverse Tasks</category>
        
        
        <category>weak supervision</category>
        
        <category>automl</category>
        
      </item>
    
      <item>
        <title>Academic Bio</title>
        <description>&lt;!--#### Hi, I&apos;m Nick!--&gt;

&lt;!--
**Academic Bio** 
--&gt;

&lt;!--
I am a Ph.D. candidate in CS at [University of Wisconsin--Madison][wisc] where I am advised by [Fred Sala][fred] along with my many talented colleagues in the [Sprocket Lab][sprocket]. 

This past Fall, I interned with the [Llama Generative AI][meta] team at Meta in London, where I worked with [Dieuwke Hupkes][dh] on language model pretraining and evaluation. 
This past Summer, I interned at [Together AI][together] in San Francisco with [Tri Dao][tri], where I worked on hybrid language models. 
In Summer 2023, I interned with the Physics of AGI research group at [Microsoft Research][msr] led by [Sébastien Bubeck][seb], where I also worked on language models.

Before starting my Ph.D., I had the pleasure of working with [Ameet Talwalkar][ameet] and [Zack Lipton][zack] during my M.S. in the Machine Learning Department at [Carnegie Mellon University][cmu]. 
As an undergraduate, I was extremely fortunate to work with both [Sanjoy Dasgupta][sanjoy] and [Gary Cottrell][gary] at the [University of California, San Diego][ucsd]. 
Prior to all of this, I was a community college student at [Fresno City College][fcc], where I was lucky enough to learn calculus, linear algebra, *and* C++ from [Greg Jamison][greg]. 

In 2025 I received an honorable mention for the [Jane Street Graduate Research Fellowship][js] and in 2023, I was named an [MLCommons][mlcommons] [Rising Star][mlsys_rising_stars]. I have also been awarded the Prove AI and UnifyID AI Fellowships in 2021 and 2019, respectively. 
--&gt;

&lt;p&gt;I am a Ph.D. candidate in Computer Science at &lt;a href=&quot;https://www.cs.wisc.edu&quot;&gt;University of Wisconsin–Madison&lt;/a&gt;, advised by &lt;a href=&quot;https://pages.cs.wisc.edu/~fredsala/&quot;&gt;Fred Sala&lt;/a&gt; in the &lt;a href=&quot;https://sprocketlab.github.io/&quot;&gt;Sprocket Lab&lt;/a&gt;, where I develop methods for efficient foundation model training and adaptation.&lt;/p&gt;

&lt;p&gt;I have completed research internships at &lt;a href=&quot;https://ai.meta.com/meta-ai/&quot;&gt;Meta’s Llama team&lt;/a&gt; (working on scaling laws with &lt;a href=&quot;https://dieuwkehupkes.nl/&quot;&gt;Dieuwke Hupkes&lt;/a&gt;), &lt;a href=&quot;https://www.together.ai/&quot;&gt;Together AI&lt;/a&gt; (hybrid language models with &lt;a href=&quot;https://tridao.me/&quot;&gt;Tri Dao&lt;/a&gt;), and &lt;a href=&quot;https://www.microsoft.com/en-us/research/&quot;&gt;Microsoft Research&lt;/a&gt; (Physics of AGI group with &lt;a href=&quot;http://sbubeck.com/&quot;&gt;Sébastien Bubeck&lt;/a&gt;). I received an honorable mention for the &lt;a href=&quot;https://www.janestreet.com/join-jane-street/programs-and-events/grf-profiles-2025/&quot;&gt;Jane Street Graduate Research Fellowship&lt;/a&gt; (2025) and was named an &lt;a href=&quot;https://mlcommons.org/en/&quot;&gt;MLCommons&lt;/a&gt; &lt;a href=&quot;https://mlcommons.org/en/news/rising-stars-2023/&quot;&gt;Rising Star&lt;/a&gt; (2023).&lt;/p&gt;

&lt;p&gt;My academic path began at &lt;a href=&quot;https://www.fresnocitycollege.edu&quot;&gt;Fresno City College&lt;/a&gt; before earning my B.S. at &lt;a href=&quot;https://ucsd.edu/&quot;&gt;UC San Diego&lt;/a&gt; (working with &lt;a href=&quot;https://cseweb.ucsd.edu/~dasgupta/&quot;&gt;Sanjoy Dasgupta&lt;/a&gt; and &lt;a href=&quot;https://cseweb.ucsd.edu/~gary/&quot;&gt;Gary Cottrell&lt;/a&gt;) and M.S. at &lt;a href=&quot;https://www.cmu.edu/&quot;&gt;Carnegie Mellon University&lt;/a&gt; (working with &lt;a href=&quot;https://www.cs.cmu.edu/~atalwalk/&quot;&gt;Ameet Talwalkar&lt;/a&gt; and &lt;a href=&quot;https://www.zacharylipton.com/&quot;&gt;Zack Lipton&lt;/a&gt;).&lt;/p&gt;

&lt;!--

&lt;hr style=&quot;height:10px;&quot;&gt;

**Research Interests** 

**My research is motivated by the need to accelerate foundation model (FM) adoption toward solving humanity&apos;s most challenging problems.** Doing so is a long-term effort requiring substantial community involvement. However, my Ph.D. research has already taken critical steps towards realizing this high-impact vision, categorized roughly into three sub-topics: 

1. The science of scaling laws,
2. Automation for improving FMs beyond naive scaling, and
3. Determining how FMs interact with data. 
   
**While furthering these directions for language, I have had the unique opportunity to pretrain LLMs at industrial scales.** On the other hand, to accelerate adoption of FMs beyond language, I have also worked with a wide array of problems from different scientific domains, which includes solving PDEs, protein folding, climate modelling, and beyond--in doing so, I helped to establish the field of *ML for diverse tasks.* 


&lt;hr style=&quot;height:10px;&quot;&gt;

--&gt;

&lt;!--
My research is motivated by the need to democratize machine learning and foundation models to handle the long tail of emerging ML tasks in the sciences and beyond. 
In order to use these models to solve high-impact problems in the sciences, my work aims to solve two main challenges: 
1. determine what additional data to provide them and understand how it interacts with pretraining data, and
2. automate the process of adapting them to new problems.
   
To address these challenges, I am focused on the intersection of data-centric ML (which aims to solve 1) and automated machine learning (AutoML; which aims to solve 2), or more concisely *data-centric AutoML*.
As a result of these motivating challenges, my work on developing the foundations of *data-centric AutoML* has a focus on diverse ML tasks that are far afield from standard ML domains.
These often include problems related to solving PDEs, protein folding, climate modeling, and beyond.
--&gt;

&lt;!--I am interested in Data-Centric AutoML--i.e., using AutoML as a data-centric tool to make machine learning more accessible and practically applicable to new domains while reducing human involvement. 
Recently, this has involved developing Data-Centric ML and AutoML techniques that lower the barrier to entry for the long tail of emerging ML applications. 
I have also developed benchmarks and competitions as a means of measuring progress on emerging ML applications that are far afield from well-explored domains in ML such as vision and language. --&gt;

</description>
        <pubDate>Sat, 09 Sep 2017 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-09/about</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-09/about</guid>
        
        
        <category>about</category>
        
      </item>
    
      <item>
        <title>Research Interests</title>
        <description>&lt;!-- **My research is motivated by the need to accelerate foundation model (FM) adoption toward solving humanity&apos;s most challenging problems.** Doing so is a long-term effort requiring substantial community involvement. The goal of my Ph.D. research is to take steps towards realizing this high-impact vision, categorized roughly into three sub-topics: 

1. The science of scaling laws,
2. Automation for improving FMs beyond naive scaling, and
3. Determining how FMs interact with data. 
   
**While furthering these directions for language, I have had the unique opportunity to pretrain LLMs at industrial scales.** On the other hand, to accelerate adoption of FMs beyond language, I have also worked with a wide array of problems from different scientific domains, which includes solving PDEs, protein folding, climate modelling, and beyond.  --&gt;

&lt;p&gt;My research develops principled methods for training and adapting foundation models to scientific domains. I focus on three interconnected areas:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;discovering optimal data-compute tradeoffs during pretraining,&lt;/li&gt;
  &lt;li&gt;automating model improvement beyond naive scaling, and&lt;/li&gt;
  &lt;li&gt;understanding how foundation models interact with specialized data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Through scaling laws research with Meta, I have shown that knowledge acquisition and reasoning tasks require fundamentally different compute allocation strategies—challenging the assumption of uniform scaling. I complement this with automated ML techniques that adapt general models to domains like protein folding and climate modeling, where labeled data is scarce and expert knowledge is critical.&lt;/p&gt;

&lt;p&gt;My goal is enabling scientists to leverage state-of-the-art AI without requiring extensive ML expertise, accelerating discovery across fields from biology to physics.&lt;/p&gt;

&lt;!--
My research is motivated by the need to democratize machine learning and foundation models to handle the long tail of emerging ML tasks in the sciences and beyond. 
In order to use these models to solve high-impact problems in the sciences, my work aims to solve two main challenges: 
1. determine what additional data to provide them and understand how it interacts with pretraining data, and
2. automate the process of adapting them to new problems.
   
To address these challenges, I am focused on the intersection of data-centric ML (which aims to solve 1) and automated machine learning (AutoML; which aims to solve 2), or more concisely *data-centric AutoML*.
As a result of these motivating challenges, my work on developing the foundations of *data-centric AutoML* has a focus on diverse ML tasks that are far afield from standard ML domains.
These often include problems related to solving PDEs, protein folding, climate modeling, and beyond.
--&gt;

&lt;!--I am interested in Data-Centric AutoML--i.e., using AutoML as a data-centric tool to make machine learning more accessible and practically applicable to new domains while reducing human involvement. 
Recently, this has involved developing Data-Centric ML and AutoML techniques that lower the barrier to entry for the long tail of emerging ML applications. 
I have also developed benchmarks and competitions as a means of measuring progress on emerging ML applications that are far afield from well-explored domains in ML such as vision and language. --&gt;

</description>
        <pubDate>Fri, 08 Sep 2017 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-09/research</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-09/research</guid>
        
        
        <category>research</category>
        
      </item>
    
      <item>
        <title>Contact</title>
        <description>&lt;b&gt;Email:&lt;/b&gt; nick11roberts [at] cs [dot] wisc [dot] edu &lt;br /&gt;
&lt;b&gt;Office:&lt;/b&gt; Morgridge Hall 5548
&lt;!-- CS Dept. 5378, 1210 W Dayton St, Madison, WI 53706 --&gt;
</description>
        <pubDate>Thu, 07 Sep 2017 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-09/contact</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-09/contact</guid>
        
        
        <category>contact</category>
        
      </item>
    
      <item>
        <title>News</title>
        <description>&lt;ul&gt;
  &lt;li&gt;Excited to visit &lt;a href=&quot;https://hazyresearch.stanford.edu/&quot;&gt;Chris Ré’s lab&lt;/a&gt; at &lt;a href=&quot;#&quot;&gt;Stanford University&lt;/a&gt; this August!&lt;/li&gt;
  &lt;li&gt;Two papers accepted to &lt;a href=&quot;#&quot;&gt;COLM 2025&lt;/a&gt;!
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00894&quot;&gt;Mancticore&lt;/a&gt; for constructing pretrained hybrids from non-hybrid base models without pretraining!&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2502.14499&quot;&gt;Meta MLGym&lt;/a&gt;: a framework for benchmarking and developing agents for AI research!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Presented at &lt;a href=&quot;#&quot;&gt;ACL 2025&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;We discover a &lt;a href=&quot;https://arxiv.org/abs/2503.10061&quot;&gt;new scaling law phenomenon&lt;/a&gt;! &lt;strong&gt;Compute optima for knowledge favor larger models, whereas those for reasoning favor more data.&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;I am extremely fortunate to have received an honorable mention for the &lt;a href=&quot;https://www.janestreet.com/join-jane-street/programs-and-events/grf-profiles-2025/&quot;&gt;Jane Street Graduate Research Fellowship&lt;/a&gt;! Thank you, &lt;a href=&quot;https://www.janestreet.com/&quot;&gt;Jane Street&lt;/a&gt;!&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
- At NeurIPS &apos;23, two brand new papers: 
  - We improve pretrained models [without any extra training or labeled data using the geometry of the label space][loki]!
  - With friends at [Hazy Research][hazy], we [teach skills to LMs][skillit], which we&apos;ll present as a spotlight! 
- Excited to have been selected as an [MLCommons Rising Star][mlsys_rising_stars]! 
- Accepted to PMLR: [AutoML Decathlon][decathlon] competition retrospective --- thanks team, and shoutout to all of our particpants! 
- Started my summer internship with the Physics of AGI group 🦄 at [Microsoft Research][msr] Redmond 
- Announcing the [AutoML Cup][automl_cup] competition! More details soon -- stay tuned! 
  - Part of the [AutoML Conference 2023][automl_conf], where I am a Competition Chair.  
- [Patent from my 2018 internship at Intuit has been issued by the US Patent Office][intuitpatent]
- At ICLR &apos;23: our work on fusing weak supervision with GANs [showcases mutual empirical and theoretical benefits][wsgan]
- At NeurIPS &apos;22, three papers, a competition, and a workshop paper: 
  - [Benchmarking neural architecture search on diverse tasks][nasbench360]
  - [Benchmark for automated weak supervision across diverse tasks][awsbench101]
  - [We lift the favorable theoretical properties of binary weak supervision to structured prediction][wssp]
  - Jointly leading the [AutoML Decathlon][decathlon] competition at NeurIPS &apos;22: an AutoML competition for diverse tasks!  
    - We are partnering with the [AutoML Fall School][automlfallschool] to host an [AutoML Decathlon][decathlon] hackathon! 
  - We call on the AutoML community to [develop automated methods aimed at high-impact climate change problems][automlccai]
- Selected for the [Jacquelin Perry][prove-fellowship] [Prove AI Fellowship!][prove]
- At ICLR &apos;22: our work on lifting weak supervision to diverse settings: [regression, rankings, manifolds, graphs, and more!][uws]
- At NeurIPS &apos;21: we re-imagine NAS operation spaces for diverse tasks: [PDE solvers, protein folding, music modeling, and beyond!][xd]
- I have started my Ph.D. at UW Madison!
--&gt;

</description>
        <pubDate>Wed, 06 Sep 2017 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-09/news</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-09/news</guid>
        
        
        <category>news</category>
        
      </item>
    
      <item>
        <title>Publications</title>
        <description>&lt;p&gt;
  &lt;h6&gt;Fresh off the Press&lt;/h6&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2506.10056&quot;&gt;&lt;b&gt;Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Gabriel Orlanski, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Aws Albarghouthi, Frederic Sala. &lt;br&gt;
      &lt;i&gt;Preprint&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2506.10056&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2505.00358&quot;&gt;&lt;b&gt;R&amp;B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Albert Ge, Tzu-Heng Huang, John Cooper, Avi Trost, Ziyi Chu, Satya Sai Srinath Namburi GNVV, Ziyang Cai, Kendall Park, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Frederic Sala. &lt;br&gt;
      &lt;i&gt;ICML 2025 DIG-BUGS Workshop&lt;/i&gt; &lt;b&gt;(oral)&lt;/b&gt;. &lt;br&gt;
      &lt;i&gt;ICML 2025 DataWorld Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2505.00358&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;
  &lt;h6&gt;Conference Publications&lt;/h6&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2406.00894&quot;&gt;&lt;b&gt;Pretrained Hybrids with MAD Skills&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Samuel Guo, Zhiqi Gao, Satya Sai Srinath Namburi GNVV, Sonia Cromp, Chengjun Wu, Chengyu Duan, Frederic Sala. &lt;br&gt;
      &lt;i&gt;COLM 2025&lt;/i&gt;. &lt;br&gt;
      &lt;!-- &lt;i&gt;ICML 2024 Long-Context Foundation Models (LCFM) Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;i&gt;ICML 2024 Next Generation of Sequence Modeling Architectures (NGSM) Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;i&gt;ICML 2024 Efficient Systems for Foundation Models (ES-FoMo) Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;i&gt;ICML 2024 Workshop on Foundation Models in the Wild&lt;/i&gt;. &lt;br&gt; --&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2406.00894&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2502.14499&quot;&gt;&lt;b&gt;MLGym: A New Framework and Benchmark for Advancing AI Research Agents&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Deepak Nathani, Lovish Madaan, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang Wang, Roberta Raileanu. &lt;br&gt;
      &lt;i&gt;COLM 2025&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2502.14499&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2503.10061&quot;&gt;&lt;b&gt;Compute Optimal Scaling of Skills: Knowledge vs Reasoning&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Niladri Chatterji, Sharan Narang, Mike Lewis, Dieuwke Hupkes. &lt;br&gt;
      &lt;i&gt;ACL Findings 2025&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://aclanthology.org/2025.findings-acl.688/&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2503.10061&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2501.0772&quot;&gt;&lt;b&gt;Stronger Than You Think: Benchmarking Weak Supervision on Realistic Tasks&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Tianyi Zhang*, Linrong Cai*, Jeffrey Li, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Neel Guha, Frederic Sala. &lt;br&gt;
      &lt;i&gt;NeurIPS 2024&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/forum?id=c7SApXZz4b#discussion&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2501.07727&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2307.12226&quot;&gt;&lt;b&gt;Geometry-Aware Adaptation for Pretrained Models&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, Frederic Sala. &lt;br&gt;
      &lt;i&gt;NeurIPS 2023&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/pdf?id=exGOXqxR0L&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2307.12226&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2307.14430&quot;&gt;&lt;b&gt;Skill-it! A data-driven skills framework for understanding and
          training language models&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Mayee Chen, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, Christopher Ré. &lt;br&gt;
      &lt;i&gt;NeurIPS 2023&lt;/i&gt; &lt;b&gt;(spotlight)&lt;/b&gt;. &lt;br&gt;
      &lt;!--&lt;i&gt;ICML 2023 Data-centric Machine Learning Research Workshop&lt;/i&gt;. &lt;br&gt;--&gt;
      &lt;a href=&quot;https://openreview.net/pdf?id=IoizwO1NLf&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2307.14430&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2203.12023&quot;&gt;&lt;b&gt;Generative Modeling Helps Weak Supervision (and Vice Versa)&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Benedikt Boecking, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Willie Neiswanger, Stefano Ermon, Frederic Sala, Artur Dubrawski. &lt;br&gt;
      &lt;i&gt;ICLR 2023&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/pdf?id=3OaBBATwsvP&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2203.12023&quot;&gt;[arXiv]&lt;/a&gt;
      &lt;a href=&quot;https://github.com/benbo/WSGAN-paper&quot;&gt;[Code]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;



&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2208.14362&quot;&gt;&lt;b&gt;AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100
          Labels&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts*&lt;/span&gt;, Xintong Li*, Tzu-Heng Huang, Dyah Adila, Spencer Schoenberg, Cheng-Yu Liu, Lauren Pick,
      Haotian
      Ma, Aws Albarghouthi, Frederic Sala. &lt;br&gt;
      &lt;i&gt;NeurIPS 2022&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/pdf?id=nQZHEunntbJ&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2208.14362&quot;&gt;[arXiv]&lt;/a&gt;
      &lt;a href=&quot;https://github.com/Sala-Group/AutoWS-Bench-101&quot;&gt;[Code]&lt;/a&gt;
      &lt;a href=&quot;https://sprocketlab.github.io/posts/2022/11/autows-bench-101/&quot;&gt;[Blog]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2110.05668&quot;&gt;&lt;b&gt;NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse
          Tasks&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Renbo Tu*, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts*&lt;/span&gt;, Mikhail Khodak, Junhong Shen, Frederic Sala, Ameet Talwalkar. &lt;br&gt;
      &lt;i&gt;NeurIPS 2022&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/pdf?id=xUXTbq6gWsB&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2110.05668&quot;&gt;[arXiv]&lt;/a&gt;
      &lt;a href=&quot;https://github.com/rtu715/NAS-Bench-360&quot;&gt;[Code]&lt;/a&gt;
      &lt;a href=&quot;https://nb360.ml.cmu.edu/&quot;&gt;[Website]&lt;/a&gt;
      &lt;a href=&quot;https://blog.ml.cmu.edu/2022/07/07/automl-for-diverse-tasks/&quot;&gt;[Blog]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2211.13375&quot;&gt;&lt;b&gt;Lifting Weak Supervision To Structured Prediction&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Harit Vishwakarma, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Frederic Sala. &lt;br&gt;
      &lt;i&gt;NeurIPS 2022&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/pdf?id=Cntmos_Ndf0&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2211.13375&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2112.03865&quot;&gt;&lt;b&gt;Universalizing Weak Supervision&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Changho Shin, Winfred Li, Harit Vishwakarma, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Frederic Sala. &lt;br&gt;
      &lt;i&gt;ICLR 2022&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/pdf?id=YpPiNigTzMT&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2112.03865&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2103.15798&quot;&gt;&lt;b&gt;Rethinking Neural Operations for Diverse Tasks&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts*&lt;/span&gt;, Mikhail Khodak*, Tri Dao, Liam Li, Christopher Ré, Ameet Talwalkar. &lt;br&gt;
      &lt;i&gt;NeurIPS 2021&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/pdf?id=je4ymjfb5LC&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2103.15798&quot;&gt;[arXiv]&lt;/a&gt;
      &lt;a href=&quot;https://github.com/nick11roberts/xd&quot;&gt;[Code]&lt;/a&gt;
      &lt;a href=&quot;https://github.com/mkhodak/relax&quot;&gt;[Software Package]&lt;/a&gt;
      &lt;a href=&quot;https://www.youtube.com/watch?v=ovpo0BdmNT4&quot;&gt;[Talk]&lt;/a&gt; &lt;br&gt;
      &lt;br&gt;
      Preliminary version: &lt;a
        href=&quot;https://www.cmu.edu/epp/patents/events/aaai21/aaaicontent/papers/searching-for-convolutions-and-a-more-ambitious-nas.pdf&quot;&gt;&lt;b&gt;Searching
          for Convolutions and a More Ambitious NAS&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts*&lt;/span&gt;, Mikhail Khodak*, Tri Dao, Liam Li, Maria-Florina Balcan, Christopher Ré, Ameet
      Talwalkar. &lt;br&gt;
      &lt;i&gt;AAAI 2021 Workshop on Learning Network Architecture During Training&lt;/i&gt; &lt;b&gt;(plenary talk)&lt;/b&gt;. &lt;br&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://papers.nips.cc/paper/7651-learning-from-discriminative-feature-feedback.pdf&quot;&gt;&lt;b&gt;Learning from
          Discriminative Feature Feedback&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Sanjoy Dasgupta, Akansha Dey, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Sivan Sabato. &lt;br&gt;
      &lt;i&gt;NeurIPS 2018&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://papers.nips.cc/paper/7651-learning-from-discriminative-feature-feedback.pdf&quot;&gt;[Paper]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;
  &lt;h6&gt;Journal Publications&lt;/h6&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;&gt;&lt;b&gt;Beyond the Imitation Game: Quantifying and extrapolating the
          capabilities of language models&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Aarohi Srivastava, ..., &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt; (276), ..., (442 authors). &lt;br&gt;
      &lt;i&gt;Transactions on Machine Learning Research (TMLR) 2023 (Finalist for Outstanding Certification)&lt;/i&gt;. &lt;br&gt;
      &lt;i&gt;ICLR 2025&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;&gt;[arXiv]&lt;/a&gt;
      &lt;a href=&quot;https://github.com/google/BIG-bench&quot;&gt;[Code]&lt;/a&gt; &lt;br&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2112.02721&quot;&gt;&lt;b&gt;NL-Augmenter: A Framework for Task-Sensitive Natural Language
          Augmentation&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Kaustubh D. Dhole, ..., &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt; (85), ..., (128 authors). &lt;br&gt;
      &lt;i&gt;Northern European Journal of Language Technology (NEJLT) 2023&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2112.02721&quot;&gt;[arXiv]&lt;/a&gt;
      &lt;a href=&quot;https://github.com/GEM-benchmark/NL-Augmenter&quot;&gt;[Code]&lt;/a&gt; &lt;br&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://proceedings.mlr.press/v220/roberts23a/roberts23a.pdf&quot;&gt;&lt;b&gt;AutoML Decathlon: Diverse Tasks, Modern Methods,
          and Efficiency at Scale&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts*&lt;/span&gt;, Samuel Guo*, Cong Xu*, Ameet Talwalkar,
      David Lander, Lvfang Tao, Linhang Cai, Shuaicheng Niu, Jianyu Heng,
      Hongyang Qin, Minwen Deng, Johannes Hog, Alexander Pfefferle, Sushil Ammanaghatta Shivakumar,
      Arjun Krishnakumar, Yubo Wang, Rhea Sukthanker, Frank Hutter, Euxhen Hasanaj, Tien-Dung Le,
      Mikhail Khodak, Yuriy Nevmyvaka, Kashif Rasul, Frederic Sala, Anderson Schneider, Junhong Shen, Evan Sparks
      &lt;br&gt;
      &lt;i&gt;PMLR NeurIPS 2022 Competition Track&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://proceedings.mlr.press/v220/roberts23a/roberts23a.pdf&quot;&gt;[Paper]&lt;/a&gt;
      &lt;a href=&quot;https://www.cs.cmu.edu/~automl-decathlon-22/&quot;&gt;[Website]&lt;/a&gt;
      &lt;a href=&quot;https://codalab.lisn.upsaclay.fr/competitions/6325&quot;&gt;[Submission Site]&lt;/a&gt;
      &lt;a href=&quot;https://github.com/cxxz/automl_decathlon_starter_kit&quot;&gt;[Code]&lt;/a&gt;
      &lt;a href=&quot;https://blog.ml.cmu.edu/2022/07/07/automl-for-diverse-tasks/&quot;&gt;[Blog]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://www.nature.com/articles/s41598-017-13923-x&quot;&gt;&lt;b&gt;Small Molecule Accurate Recognition Technology
          (SMART) to Enhance Natural Products Research&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Chen Zhang*, Yerlan Idelbayev*, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Yiwen Tao, Yashwanth Nannapaneni, Brendan M. Duggan, Jie
      Min,
      Eugene C. Lin, Erik C. Gerwick, Garrison W. Cottrell, William H. Gerwick. &lt;br&gt;
      &lt;i&gt;Scientific Reports 2017&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://www.nature.com/articles/s41598-017-13923-x&quot;&gt;[Paper]&lt;/a&gt; &lt;br&gt;
      &lt;br&gt;
      Poster: &lt;b&gt;Small Molecule Accurate Recognition Technology (SMART): A Digital Frontier to Reshape Natural Product
        Research&lt;/b&gt; &lt;br&gt;
      Chen Zhang*, Yerlan Idelbayev*, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts (presenter)&lt;/span&gt;, Yiwen Tao, Yashwanth Nannapaneni, Brendan M.
      Duggan,
      Jie Min, Eugene C. Lin, Erik C. Gerwick, Garrison W. Cottrell, William H. Gerwick. &lt;br&gt;
      Best Spotlight Presentation Award: &lt;i&gt;Applied Machine Learning Days 2018&lt;/i&gt;. &lt;br&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
  &lt;h6&gt;Workshop Publications and Preprints&lt;/h6&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;#&quot;&gt;&lt;b&gt;Tabby: Tabular Adaptation for Language Models&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Sonia Cromp, Satya Sai Srinath Namburi GNVV, Catherine Cao, Mohammed Alkhudhayri, Samuel Guo, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Frederic Sala &lt;br&gt;
      &lt;i&gt;NeurIPS 2024 Table Representation Learning Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://openreview.net/forum?id=gh3WrztrNC&quot;&gt;[Paper]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2408.17383&quot;&gt;&lt;b&gt;MoRe Fine-Tuning with 10x Fewer Parameters&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Wenxuan Tan, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Tzu-Heng Huang, Jitian Zhao, John Cooper, Samuel Guo, Chengyu Duan, Frederic Sala. &lt;br&gt;
      &lt;i&gt;ICML 2024 Efficient Systems for Foundation Models (ES-FoMo) Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;i&gt;ICML 2024 Workshop on Foundation Models in the Wild&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2408.17383&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;/assets/NAS-theory.pdf&quot;&gt;&lt;b&gt;Understanding Neural Architecture Search by its
          Architecture Parameters&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Yingyu Liang, Frederic Sala. &lt;br&gt;
      &lt;i&gt;Midwest Machine Learning Symposium 2023&lt;/i&gt;. &lt;br&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;#&quot;&gt;&lt;b&gt;ScriptoriumWS: A Code Generation Assistant for Weak Supervision&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Tzu-Heng Huang, Harit Vishwakarma, Catherine Cao, Spencer Schoenberg, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Frederic Sala. &lt;br&gt;
      &lt;i&gt;ICLR 2023 Deep Learning for Code Workshop&lt;/i&gt;. &lt;br&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2210.03324&quot;&gt;&lt;b&gt;AutoML for Climate Change: A Call to Action&lt;/b&gt;&lt;/a&gt;
      &lt;br&gt;
      Renbo Tu, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Vishak Prasad, Sibasis Nayak, Paarth Jain, Frederic Sala, Ganesh Ramakrishnan,
      Ameet Talwalkar, Willie Neiswanger, Colin White. &lt;br&gt;
      &lt;i&gt;NeurIPS 2022 Tackling Climate Change with Machine Learning Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2210.03324&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2011.13477&quot;&gt;&lt;b&gt;Decoding and Diversity in Machine Translation&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Davis Liang, Graham Neubig, Zachary C. Lipton. &lt;br&gt;
      &lt;i&gt;NeurIPS 2020 Resistance AI Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2011.13477&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_46.pdf&quot;&gt;&lt;b&gt;A Simple Setting for
          Understanding Neural Architecture Search with Weight-Sharing&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Mikhail Khodak, Liam Li, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Maria-Florina Balcan, Ameet Talwalkar. &lt;br&gt;
      &lt;i&gt;ICML 2020 AutoML Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_46.pdf&quot;&gt;[Paper]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://liamcli.com/assets/pdf/weight_sharing.pdf&quot;&gt;&lt;b&gt;Weight-Sharing Beyond Neural Architecture Search:
          Efficient Feature Map Selection and Federated Hyperparameter Tuning&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Mikhail Khodak*, Liam Li*, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Maria-Florina Balcan, Ameet Talwalkar. &lt;br&gt;
      &lt;i&gt;MLSys 2020 On-Device Intelligence Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://liamcli.com/assets/pdf/weight_sharing.pdf&quot;&gt;[Paper]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/1912.08986&quot;&gt;&lt;b&gt;Deep Connectomics Networks: Neural Network Architectures Inspired by
          Neuronal Networks&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Dian Ang Yap, Vinay U. Prabhu. &lt;br&gt;
      &lt;i&gt;NeurIPS 2019 Real Neurons and Hidden Units Workshop&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/1912.08986&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://ml4physicalsciences.github.io/2019/files/NeurIPS_ML4PS_2019_139.pdf&quot;&gt;&lt;b&gt;Using Deep Siamese Neural
          Networks to Speed up Natural Products Research&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Poornav S. Purushothama, Vishal T. Vasudevan, Siddarth Ravichandran, Chen Zhang, William
      H.
      Gerwick, Garrison W. Cottrell. &lt;br&gt;
      &lt;i&gt;NeurIPS 2019 workshop on Machine Learning and the Physical Sciences&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://ml4physicalsciences.github.io/2019/files/NeurIPS_ML4PS_2019_139.pdf&quot;&gt;[Paper]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/1911.07418&quot;&gt;&lt;b&gt;Grassmannian Packings in Neural Networks: Learning with Maximal
          Subspace Packings for Diversity and Anti-Sparsity&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      Dian Ang Yap, &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Vinay U. Prabhu. &lt;br&gt;
      &lt;i&gt;NeurIPS 2019 workshop on Bayesian Deep Learning&lt;/i&gt;. &lt;br&gt;
      &lt;i&gt;NeurIPS 2019 workshop on Information Theory and Machine Learning&lt;/i&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/1911.07418&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://arxiv.org/abs/1912.08987&quot;&gt;&lt;b&gt;Model Weight Theft With Just Noise Inputs: The Curious Case of the
          Petulant Attacker&lt;/b&gt;&lt;/a&gt; &lt;br&gt;
      &lt;span class=&quot;author-highlight&quot;&gt;Nicholas Roberts&lt;/span&gt;, Vinay U. Prabhu, Matthew McAteer. &lt;br&gt;
      &lt;i&gt;ICML 2019 workshop on Security and Privacy of Machine Learning&lt;/i&gt; &lt;b&gt;(spotlight)&lt;/b&gt;. &lt;br&gt;
      &lt;a href=&quot;https://arxiv.org/abs/1912.08987&quot;&gt;[arXiv]&lt;/a&gt;
    &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 05 Sep 2017 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-09/publications</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-09/publications</guid>
        
        
        <category>publications</category>
        
      </item>
    
      <item>
        <title>Education</title>
        <description>&lt;h4&gt;University of Wisconsin–Madison&lt;/h4&gt;
&lt;!--&lt;a href=&quot;https://www.cs.wisc.edu&quot;&gt;
&lt;img height=&quot;60px&quot; src=&quot;/img/uw.png&quot;/&gt;
&lt;/a&gt;--&gt;
&lt;h6&gt;Ph.D. Computer Science&lt;br/&gt;
Mathematics minor&lt;/h6&gt;
August 2021 - Present
&lt;ul&gt;
&lt;li&gt;
Visiting student with &lt;a href=&quot;#&quot;&gt;Chris Ré&apos;s lab&lt;/a&gt; at &lt;a href=&quot;#&quot;&gt;Stanford University&lt;/a&gt; (August 2025) 
&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
Member of the &lt;a href=&quot;https://wisconsintriathlonteam.weebly.com/&quot;&gt;Wisconsin Triathlon Team&lt;/a&gt;&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
Member of the &lt;a href=&quot;https://www.hoofersailing.org/&quot;&gt;Hoofer Sailing Club&lt;/a&gt;&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://www.cs.wisc.edu/catapult-clubs/&quot;&gt;Scratch Club&lt;/a&gt; volunteer (teaching CS to Madison area 4th-5th graders)&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;



&lt;h4&gt;Carnegie Mellon University&lt;/h4&gt;
&lt;!--&lt;a href=&quot;https://www.cmu.edu/&quot;&gt;
&lt;img height=&quot;30px&quot; src=&quot;/img/cmu.png&quot;/&gt;
&lt;/a&gt;--&gt;
&lt;h6&gt;M.S. Machine Learning&lt;/h6&gt;
August 2019 - May 2021
&lt;ul&gt;
&lt;li&gt;
MSML Student Committee Leader&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
MSML Admissions Committee Member&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;


&lt;h4&gt;University of California, San Diego&lt;/h4&gt;
&lt;!--&lt;a href=&quot;https://www.ucsd.edu/&quot;&gt;
&lt;img height=&quot;30px&quot; src=&quot;/img/ucsd.png&quot;/&gt;
&lt;/a&gt;--&gt;
&lt;h6&gt;B.S. Computer Science&lt;br/&gt;
Mathematics minor&lt;br/&gt;
CSE Honors Program&lt;/h6&gt;
September 2015 - March 2019&lt;br/&gt;
Magna Cum Laude and CSE Highest Distinction honors
&lt;ul&gt;
&lt;li&gt;
Tutor for Data Science 10 (Principles of Data Science)&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
Tutor for Data Science 20 (Data Structures for Data Science)&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
Data analyst, &lt;a href=&quot;https://tesc.ucsd.edu/&quot;&gt;Triton Engineering Student Council&lt;/a&gt;&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
Machine learning workshop facilitator, &lt;a href=&quot;https://ds3.ucsd.edu/&quot;&gt;Data Science Student Society at UCSD&lt;/a&gt;&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
House leader, &lt;a href=&quot;https://tbp.ucsd.edu/&quot;&gt;Tau Beta Pi, California Psi&lt;/a&gt;&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;

&lt;h4&gt;Fresno City College&lt;/h4&gt;
&lt;!--&lt;a href=&quot;https://www.fresnocitycollege.edu/&quot;&gt;
&lt;img height=&quot;30px&quot; src=&quot;/img/fcc.png&quot;/&gt;
&lt;/a&gt;--&gt;
&lt;h6&gt;Computer Science&lt;br/&gt;
Leon S. Peters Honors Program&lt;/h6&gt;
August 2013 - May 2015
&lt;ul&gt;
&lt;li&gt;
Tutor for CIT 65 (Android Application Development)&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
Mathematics Tutor&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
Computer Science Tutor&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
President/Founder, Google Developer Group Fresno City College&lt;br/&gt;
&lt;/li&gt;
&lt;li&gt;
Treasurer, Science and Engineering Club&lt;br/&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
</description>
        <pubDate>Mon, 04 Sep 2017 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-09/education</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-09/education</guid>
        
        
        <category>education</category>
        
      </item>
    
      <item>
        <title>Industry Experience</title>
        <description>&lt;!--&lt;h4&gt;Sala Group&lt;/h4&gt; &lt;code&gt;research assistant &lt;br/&gt;(Madison, WI)&lt;/code&gt;
&lt;a href=&quot;https://www.cs.wisc.edu&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/uw.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Research related to Weak Supervision and Automated Machine Learning supervised by Fred Sala.
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;SAGE Lab&lt;/h4&gt; &lt;code&gt;research assistant &lt;br/&gt;(Pittsburgh, PA)&lt;/code&gt;
&lt;a href=&quot;https://www.cs.cmu.edu/~atalwalk/group.html&quot;&gt;
  &lt;img height=&quot;30px&quot; src=&quot;/img/cmu.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Research related to Neural Architecture Search supervised by Ameet Talwalkar
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt; --&gt;


&lt;h4&gt;Meta AI&lt;/h4&gt; &lt;h6&gt;Research Scientist Intern 🦙&lt;br/&gt;(London, UK)&lt;/h6&gt;
&lt;a href=&quot;https://research.facebook.com/&quot;&gt;
  &lt;img height=&quot;140px&quot; src=&quot;/img/meta.svg&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Llama Generative AI pretraining team with &lt;a href=&quot;https://dieuwkehupkes.nl/&quot;&gt;Dieuwke Hupkes&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    Discovered a new phenomenon: knowledge and reasoning skills have different scaling behavior 
  &lt;/li&gt;
  &lt;li&gt;
    Incorporated the AUP score, originally used in our AutoML Decathlon competition, into Meta MLGym 
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, PyTorch 
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;Together AI&lt;/h4&gt; &lt;h6&gt;Research Intern 🐍&lt;br/&gt;(San Francisco, CA)&lt;/h6&gt;
&lt;a href=&quot;https://www.together.ai/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/together.svg&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Research with &lt;a href=&quot;https://tridao.me/&quot;&gt;Tri Dao&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    Analyzed the role of specific attention heads for long-range retrieval tasks in hybrid LLMs 
  &lt;/li&gt;
  &lt;li&gt;
    Investigated the mechanisms behind what makes hybrid LLMs good at in-context recall 
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, PyTorch, HuggingFace
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;


&lt;h4&gt;Microsoft Research&lt;/h4&gt; &lt;h6&gt;Research Intern 🦄&lt;br/&gt;(Redmond, WA)&lt;/h6&gt;
&lt;a href=&quot;https://www.microsoft.com/en-us/research/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/msr.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Physics of AGI research group led by &lt;a href=&quot;http://sbubeck.com/&quot;&gt;Sébastien Bubeck&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    Developed activation function search techniques for large-scale LLM pretraining
  &lt;/li&gt;
  &lt;li&gt;
    Developed learning curve extrapolation techniques to ablate architectural choices in transformers
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, PyTorch, HuggingFace
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;Amazon AI&lt;/h4&gt; &lt;h6&gt;Applied Scientist Intern &lt;br/&gt;(Seattle, WA)&lt;/h6&gt;
&lt;a href=&quot;https://aws.amazon.com/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/amazon_full.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    AWS Transcribe research group led by &lt;a
      href=&quot;https://www.linkedin.com/in/katrin-kirchhoff-19388049/&quot;&gt;Katrin Kirchhoff&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    Researched and developed methods for hypothesis rescoring in ASR systems using neural language modeling
  &lt;/li&gt;
  &lt;li&gt;
    Identified areas for improvement in many existing ASR systems when recognizing rare or zero shot entities
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, PyTorch, RWTH ASR, Kaldi, AWS
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;UnifyID&lt;/h4&gt; &lt;h6&gt;AI Fellow &lt;br/&gt; Machine Learner Intern &lt;br/&gt;(Redwood City, CA)&lt;/h6&gt;
&lt;a href=&quot;https://unify.id/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/unifyid.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    UnifyID research lab led by &lt;a href=&quot;https://www.vinayprabhu.com/&quot;&gt;Vinay Uday Prabhu&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    Researched various ways in which research from network neuroscience could be applied to deep learning
  &lt;/li&gt;
  &lt;li&gt;
    Developed a novel model extraction attack against deep learning models for computer vision using just noise inputs
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, Keras, PyTorch, TensorFlow, MATLAB, AWS
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;Intuit&lt;/h4&gt; &lt;h6&gt;Software Engineering Intern &lt;br/&gt;(Mountain View, CA)&lt;/h6&gt;
&lt;a href=&quot;https://www.intuit.com&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/intuit.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Intuit Technology Futures research group
  &lt;/li&gt;
  &lt;li&gt;
    Researched and implemented a novel deep learning model for controllable text generation as a service within Intuit
  &lt;/li&gt;
  &lt;li&gt;
    Developed a system for proposing alternative candidate sentences for Intuit content writers using deep learning
  &lt;/li&gt;
  &lt;li&gt;
    Investigated the use of dynamic topic models for customer support tickets to gain actionable insights over time
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, PyTorch, TensorFlow, Gensim, Keras
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;Altum&lt;/h4&gt; &lt;h6&gt;Applied Scientist Intern &lt;br/&gt;(La Jolla, CA)&lt;/h6&gt;
&lt;a href=&quot;https://altum.io/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/altum.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Developed language model to extract NLP features from text data regarding cryptocurrency trading
  &lt;/li&gt;
  &lt;li&gt;
    Investigated unsupervised learning techniques for extracting sentiment data in real time from online forums
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, PyTorch
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;!--
&lt;h4&gt;The Cottrell Lab (GURU: Gary&apos;s Unbelievable Research Unit)&lt;/h4&gt; &lt;code&gt;undergraduate researcher &lt;br/&gt;(La Jolla, CA)&lt;/code&gt;
&lt;a href=&quot;https://cseweb.ucsd.edu/groups/guru/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/guru.gif&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Published, &lt;a href=&quot;https://www.nature.com/srep/&quot;&gt;Scientific Reports&lt;/a&gt;:
  &lt;/li&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &quot;Small Molecule Accurate Recognition Technology (SMART) to Enhance Natural Products Research&quot;
    &lt;/li&gt;
  &lt;/ul&gt;
  &lt;li&gt;
    Presented SMART research at the &lt;a href=&quot;https://www.appliedmldays.org/&quot;&gt;Applied Machine Learning Days 2018&lt;/a&gt;
    conference at &lt;a href=&quot;https://epfl.ch/&quot;&gt;EPFL&lt;/a&gt;, Lausanne, Switzerland
    &lt;ul&gt;
      &lt;li&gt;
        Won an award for Best Spotlight Presentation
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    Analyzed performance of deep learning system for natural products research with &lt;a
      href=&quot;https://scripps.ucsd.edu/&quot;&gt;Scripps Institute of Oceanography&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    Explored the effects of artificial experimental noise added to the dataset and showed resistance to gaussian noise
  &lt;/li&gt;
  &lt;li&gt;
    Improved quality of image dataset by identifying and handling noisy outliers using principal component analysis
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, Tensorflow, Lasagne, Theano, SciPy
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;
--&gt;

&lt;h4&gt;Teradata&lt;/h4&gt; &lt;h6&gt;Software Engineering Intern &lt;br/&gt;(San Diego, CA)&lt;/h6&gt;
&lt;a href=&quot;https://www.teradata.com/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/teradata.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Developed open source Spark-Teradata connector forked from Databricks’ connector for AWS Redshift in Scala
  &lt;/li&gt;
  &lt;li&gt;
    Designed and implemented Teradata stored procedures in Java to mimic Redshift’s UNLOAD and COPY using S3
  &lt;/li&gt;
  &lt;li&gt;
    Improved training methodology and architecture of deep learning time series model used internally
  &lt;/li&gt;
  &lt;li&gt;
    Implemented system for updating the time series dataset and fine tuning the deep learning model
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Scala, Java, Maven, Teradata SQL, AWS, Tensorflow, Flask
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;The Comeback Community&lt;/h4&gt; &lt;h6&gt;Volunteer Full Stack Developer &lt;br/&gt;(Remote)&lt;/h6&gt;
&lt;a href=&quot;https://the-comeback-community.appspot.com/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/comeback.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Developed site in Go, gohtml, and CSS on Google App Engine
  &lt;/li&gt;
  &lt;li&gt;
    Mentored new developers in web development
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Go, Google App Engine, gohtml, HTML5, CSS3, JavaScript
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;Skqrl&lt;/h4&gt; &lt;h6&gt;Software Engineering Intern &lt;br/&gt;(La Jolla, CA)&lt;/h6&gt;
&lt;a href=&quot;https://skqrl.com/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/skqrl.png&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Developed web crawler to compile needfinding and product data using Scrapy and Selenium
  &lt;/li&gt;
  &lt;li&gt;
    Designed and implemented an extensible product search solution designed to handle future user search needs
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Python, Scrapy, Selenium, Django, MySQL, JavaScript
  &lt;/li&gt;
&lt;/ul&gt;
&lt;br /&gt;

&lt;h4&gt;ModSpot&lt;/h4&gt; &lt;h6&gt;Software Engineering Intern &lt;br/&gt;(Remote)&lt;/h6&gt;
&lt;a href=&quot;https://modspotapp.wordpress.com/&quot;&gt;
  &lt;img height=&quot;70px&quot; src=&quot;/img/modspot.jpg&quot; /&gt;
&lt;/a&gt;
&lt;ul&gt;
  &lt;li&gt;
    Implemented new user account, edit profile, and login designs in Objective-C for iOS application
  &lt;/li&gt;
  &lt;li&gt;
    Refactored analytics code for gathering statistics on app usage, helping designers make more informed choices
  &lt;/li&gt;
  &lt;li&gt;
    Technologies used: Objective-C, Cocoa Touch, Flurry Analytics
  &lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
</description>
        <pubDate>Sun, 03 Sep 2017 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-09/experience</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-09/experience</guid>
        
        
        <category>experience</category>
        
      </item>
    
      <item>
        <title>Fun</title>
        <description>&lt;p&gt;&lt;strong&gt;Extracurricular interests:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Aspiring triathlete&lt;/li&gt;
  &lt;li&gt;Sailing&lt;/li&gt;
  &lt;li&gt;Pottery&lt;/li&gt;
  &lt;li&gt;Guitar&lt;/li&gt;
  &lt;li&gt;Interior design&lt;/li&gt;
  &lt;li&gt;A budding interest in plants&lt;/li&gt;
  &lt;li&gt;Longboard construction and woodworking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Extra-Extracurricular interests:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://thelevermag.com/&quot;&gt;Into vintage lever espresso machines,&lt;/a&gt;&lt;/em&gt; and proud owner of a 1974 Olympia Cremina (sans asbestos, of course).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://dudeism.com&quot;&gt;Ordained Dudeist priest.&lt;/a&gt;&lt;/em&gt; I can legally officiate weddings in the US. If you’d like to book me for your wedding, please feel free to reach out.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://www.vacation.inc/?referredBy=roberts88365&quot;&gt;Head Researcher of Margarita Machine Lounge Therapy at Vacation Inc.&lt;/a&gt;&lt;/em&gt; Check out our selection of luxury sunscreens today! &lt;a href=&quot;https://www.vacation.inc/?referredBy=roberts88365&quot;&gt;And use my referral link!&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://www.instagram.com/p/CQIVWAxg4NX/&quot;&gt;Unofficial Toyota Prius landspeed record holder at Bonneville Speedway.&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://www.instagram.com/p/CmAOIuKOZCF/&quot;&gt;Trying to get into Iceboat racing.&lt;/a&gt;&lt;/em&gt; Personal goal: become the fastest Iceboat racer to ever hail from Fresno, CA (a low bar, indeed).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Did I mention I’m into espresso? &lt;a href=&quot;https://github.com/Zer0-bit/gaggiuino&quot;&gt;I added a microcontroller to a cheap espresso machine&lt;/a&gt; to elevate it to the level of a &lt;a href=&quot;https://decentespresso.com/&quot;&gt;$4,000 Decent&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Credentials:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img width=&quot;300px&quot; src=&quot;/img/vacation_inc.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img width=&quot;300px&quot; src=&quot;/img/my-poolsuite-card.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img width=&quot;300px&quot; src=&quot;/img/dude.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Photograph of me shredding, circa 2009:&lt;/strong&gt; 
&lt;br /&gt;&lt;br /&gt;
&lt;img width=&quot;300px&quot; src=&quot;/img/guitar.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corn, circa long ago, colorized:&lt;/strong&gt; 
&lt;br /&gt;&lt;br /&gt;
&lt;img width=&quot;300px&quot; src=&quot;/img/corn.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mixtape:&lt;/strong&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;iframe allow=&quot;autoplay *; encrypted-media *; fullscreen *; clipboard-write&quot; frameborder=&quot;0&quot; height=&quot;450&quot; style=&quot;width:100%;max-width:660px;overflow:hidden;background:transparent;&quot; sandbox=&quot;allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation&quot; src=&quot;https://embed.music.apple.com/us/playlist/mixtape/pl.u-oZylYeRTRMNyYrV&quot;&gt;
&lt;/iframe&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Don’t forget to check out &lt;a href=&quot;https://poolsuite.net/&quot;&gt;Poolsuite FM&lt;/a&gt;: the ultra-summer music player for the Macintosh Computer; transporting you to a virtual vacation where the sun never sets.&lt;/p&gt;

&lt;!--DnD class (homebrew rules): `Wizard/Bard/Cobbler` hybrid.  --&gt;
&lt;!--&quot;`My key to dealing with stress is simple:` `just stay cool and stay focused.`&quot; -Ashton Eaton (cheesy quote courtesy of the first Google search result for &apos;cool quotes&apos;).--&gt;

</description>
        <pubDate>Sat, 02 Sep 2017 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-09/fun</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-09/fun</guid>
        
        
        <category>fun</category>
        
      </item>
    
  </channel>
</rss>
